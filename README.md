# ğŸ’ Monkey Troop

**Decentralized AI Compute Grid**

Monkey Troop is a FOSS (MIT Licensed) peer-to-peer network that democratizes access to AI inference. Users donate idle GPU time to run LLM inference for others in exchange for time-based credits, similar to folding@home but for AI.

## ğŸŒŸ Vision

Enable anyone to:
- **Donate** idle GPU compute when not in use locally
- **Earn** credits based on actual hardware performance (time-based, normalized)
- **Use** those credits to access high-performance GPUs when needed
- **Trust** the network through cryptographic verification and proof-of-hardware

### Key Features

- **ğŸ”’ Secure P2P Mesh**: Direct node-to-node connections via Tailscale/Headscale (WireGuard)
- **ğŸ¯ OpenAI Compatible**: Drop-in replacement for any tool using OpenAI API
- **âš–ï¸ Fair Economy**: Time-based credits with hardware multipliers (RTX 4090 = 4x, etc.)
- **ğŸ” Proof-of-Hardware**: Cryptographic benchmarking prevents hardware spoofing
- **ğŸ¤ Trusted Clusters**: Create private networks with friends/teams
- **ğŸŒ Public Commons**: Join the global network at `troop.100monkeys.ai`
- **ğŸ”§ Multi-Engine**: Supports Ollama, LM Studio, vLLM, and more

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Coordinator    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Worker    â”‚
â”‚  (Sidecar)  â”‚ Tickets â”‚ (troop.monkey.ai)â”‚Discoveryâ”‚   (Agent)   â”‚
â”‚             â”‚         â”‚                  â”‚         â”‚             â”‚
â”‚ localhost:  â”‚         â”‚ - Redis Registry â”‚         â”‚ - Ollama    â”‚
â”‚   9000      â”‚         â”‚ - PostgreSQL     â”‚         â”‚ - Tailscale â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ - JWT Auth       â”‚         â”‚ - GPU       â”‚
      â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                                      â–²
      â”‚                 Direct P2P Connection               â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (Encrypted via Tailscale WireGuard)
```

### Components

1. **Coordinator** (Python/FastAPI): Discovery, authentication, proof-of-hardware verification
2. **Worker** (Rust): GPU monitoring, heartbeat broadcasting, JWT verification proxy
3. **Client** (Rust): Local OpenAI-compatible API proxy for seamless integration
4. **Shared** (Rust): Common data structures and types

## ğŸš€ Quick Start

There are **two installation paths** depending on your role:

### ğŸ‘¥ End Users: Join a Network

Install the worker (to donate GPU) or client (to use GPU) to join an existing network.

```bash
# Install worker/client binaries
curl -fsSL https://raw.githubusercontent.com/monkeytroop/monkey-troop/main/install.sh | bash

# Join the public network
export COORDINATOR_URL="https://troop.100monkeys.ai/api"
tailscale up --login-server=https://troop.100monkeys.ai/vpn --authkey=<provided-key>

# Start donating compute
monkey-troop-worker

# OR use the network (in another terminal)
monkey-troop-client
# Point your AI tool to: http://localhost:9000/v1
```

### ğŸ¢ Network Operators: Deploy a Coordinator Hub

Deploy your own coordinator with Headscale VPN for a private network.

```bash
# Clone repository on your VPS
git clone https://github.com/monkeytroop/monkey-troop.git
cd monkey-troop

# Run automated installer (interactive)
./install-coordinator.sh

# OR with command-line flags
./install-coordinator.sh \
  --domain troop.example.com \
  --email admin@example.com \
  --routing-mode path \
  --enable-backups
```

**What gets installed:**
- âœ… Headscale VPN server (node discovery)
- âœ… Coordinator API (FastAPI + PostgreSQL + Redis)
- âœ… Caddy reverse proxy (automatic HTTPS)
- âœ… Systemd services (auto-restart)
- âœ… Optional: Daily database backups

See [DEPLOYMENT.md](docs/DEPLOYMENT.md) for detailed documentation.

---

### Self-Host a Private Cluster (Manual)

For advanced users who want full control, see [DEPLOYMENT.md](docs/DEPLOYMENT.md) for manual Headscale setup instructions.

## ğŸ› ï¸ Development

### Prerequisites

- **Rust** 1.75+ (for worker/client)
- **Python** 3.11+ (for coordinator)
- **Docker** & Docker Compose
- **PostgreSQL** 15+
- **Redis** 7+

### Build from Source

```bash
# Clone the repository
git clone https://github.com/monkeytroop/monkey-troop.git
cd monkey-troop

# Build Rust components
cargo build --release

# Set up Python environment
cd coordinator
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Run Locally

```bash
# Start coordinator stack
docker-compose -f docker-compose.coordinator.yml up -d

# Run worker (requires GPU)
cargo run --bin monkey-troop-worker

# Run client
cargo run --bin monkey-troop-client
```

### Using Streaming

Enable streaming responses for real-time token generation:

```python
import requests

response = requests.post(
    "http://localhost:3000/v1/chat/completions",
    json={
        "model": "llama3:8b",
        "messages": [{"role": "user", "content": "Write a story"}],
        "stream": True
    },
    stream=True
)

for chunk in response.iter_lines():
    if chunk:
        print(chunk.decode('utf-8'))
```

### Multi-Engine Support

Monkey Troop automatically detects and supports multiple inference engines:

- **vLLM** (highest priority - fastest inference)
- **Ollama** (versatile, easy setup)
- **LM Studio** (GUI-based management)

Workers detect all available engines at startup and route requests intelligently based on model availability. vLLM models are prioritized for performance.

**Setup vLLM** (optional):
```bash
# Install vLLM
pip install vllm

# Start vLLM server
vllm serve meta-llama/Llama-3-8B --port 8000

# Or use custom host
export VLLM_HOST=http://localhost:8000
```

**Configure model refresh** (optional):
```bash
# Check for new models every 5 minutes (default: 3 minutes)
export MODEL_REFRESH_INTERVAL=300
```

## ğŸ“– Documentation

- [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md) - Deploy your own Headscale coordinator
- [CONTRIBUTING.md](CONTRIBUTING.md) - Development setup and guidelines
- [docs/PROJECT_STRUCTURE.md](docs/PROJECT_STRUCTURE.md) - Project architecture details
- [docs/MVP_STATUS.md](docs/MVP_STATUS.md) - Implementation status and roadmap
- [docs/TESTING_GUIDE.md](docs/TESTING_GUIDE.md) - Testing instructions

## ğŸ¤ Contributing

Monkey Troop is fully open source (MIT License). Contributions are welcome!

See [CONTRIBUTING.md](CONTRIBUTING.md) for development setup.

## ğŸ“œ License

MIT License - Copyright (c) 2026 Monkey Troop Contributors

## ğŸ™ Acknowledgments

Inspired by:
- [Petals](https://github.com/bigscience-workshop/petals) - Distributed inference concepts
- [Folding@home](https://foldingathome.org/) - Distributed computing for good
- [Ollama](https://ollama.ai/) - Local LLM runtime
- [Tailscale](https://tailscale.com/) - Zero-config VPN mesh networking

---

**ğŸš¨ Status**: Phase 2 Complete - Production-Ready Alpha (93.8%)

The system includes:
- âœ… Credit accounting with PostgreSQL ledger
- âœ… Rate limiting (100/hr default, 20/hr strict)
- âœ… Audit logging to PostgreSQL
- âœ… JWT-based authorization (RSA-2048)
- âœ… Proof-of-Hardware benchmarking
- âœ… Timeout enforcement (5s/30s/300s)
- âœ… Streaming responses (Server-Sent Events)
- âœ… Multi-engine support (Ollama, vLLM, LM Studio)
- âœ… Integration tests + CI/CD pipeline
- ğŸš§ VPS deployment (handled separately)

See [docs/MVP_STATUS.md](docs/MVP_STATUS.md) for detailed progress.

Join us in building the future of decentralized AI!
